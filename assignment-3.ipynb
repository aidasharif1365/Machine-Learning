{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Gradient Descent and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (5 points)\n",
    "\n",
    "Let's implement a test function for the gradient descent optimizer, a 3D simple parabola. All gradient-optimized trainers are implemented as a objective function. The follow the same basic pattern:\n",
    "\n",
    "```ruby\n",
    "class MyGradientLearnableModel\n",
    "  def func x, w\n",
    "    #Returns the value of the objective function, \n",
    "    #  summing across all examples in x\n",
    "  end\n",
    "  def grad x, w\n",
    "    #Returns a hash of derivative values for each variable in w,\n",
    "    # gradient is summed across all examples in x\n",
    "    dw = {\"0\" => (w[\"0\"] - 1), \"1\" => (w[\"1\"] - 2)}\n",
    "  end\n",
    "  def adjust w\n",
    "    # Applies any problem-specific alterations to w\n",
    "  end\n",
    "end\n",
    "```\n",
    "\n",
    "Now, let's implement a Parabola objective function which does not depend on the data at all. It is defined as follows:\n",
    "\n",
    "### $L(w) = \\frac{1}{2}\\left( \\left(w_{0} - 1\\right)^2 + \\left(w_{1} - 2\\right)^2 \\right)$\n",
    "\n",
    "Implement the ```func``` method for the loss function, $L(w)$. \n",
    "\n",
    "**Note**: The data, i.e., ```x``` is not actually used in this objective function. Other objective functions may use ```x```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54115f66272f6d5339cda445dcca261d",
     "grade": false,
     "grade_id": "cell-f52748ce9cfc537f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParabolaObjective\n",
    "  def func x, w\n",
    "    sum=1.0/2.0*((w[\"0\"]-1)**2+(w[\"1\"]-2)**2)\n",
    "    return sum\n",
    "  end\n",
    "  def adjust w\n",
    "    return w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f4ac4ef8ff6b09de9799e7aada958998",
     "grade": true,
     "grade_id": "cell-91665d329bcaad34",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "t1 = ParabolaObjective.new\n",
    "assert_in_delta(0.0, t1.func([], {\"0\" => 1.0, \"1\" => 2.0}), 1e-3)\n",
    "assert_in_delta(0.5, t1.func([], {\"0\" => 1.0, \"1\" => 1.0}), 1e-3)\n",
    "assert_in_delta(0.5, t1.func([], {\"0\" => 1.0, \"1\" => 3.0}), 1e-3)\n",
    "assert_in_delta(2.5, t1.func([], {\"0\" => 3.0, \"1\" => 1.0}), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 (5 Points)\n",
    "\n",
    "Implement the gradient function for $L(w)$. It evaluates the gradient for the value of $x$ for each dimension of $w$. In this simple case, $L(w)$ does not depend on $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "336415156704e9b2ea16ae3940b2e032",
     "grade": false,
     "grade_id": "cell-b5c9699ed1ed88fa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParabolaObjective\n",
    "  def grad x, w\n",
    "    dw = {\"0\" => (w[\"0\"] - 1).to_f, \"1\" => (w[\"1\"] - 2).to_f}\n",
    "    return dw\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9b03b5652d757511bb12c6642715b2d",
     "grade": true,
     "grade_id": "cell-aaa3cd8bce5fe79b",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "t2 = ParabolaObjective.new\n",
    "w2_1 = t2.grad([], {\"0\" => 3.0, \"1\" => 7.0})\n",
    "assert_in_delta(2.0, w2_1[\"0\"], 1e-3)\n",
    "assert_in_delta(5.0, w2_1[\"1\"], 1e-3)\n",
    "\n",
    "w2_2 = t2.grad([], {\"0\" => -3.0, \"1\" => -7.0})\n",
    "assert_in_delta(-4.0, w2_2[\"0\"], 1e-3)\n",
    "assert_in_delta(-9.0, w2_2[\"1\"], 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 (1 Point)\n",
    "\n",
    "\n",
    "Implement gradient descent for any objective function class. Your function must provide a callback which we will use to monitor its performance and possibly to halt execution. A simple example illustrating the callback is as follows:\n",
    "\n",
    "```ruby\n",
    "def gradient_descent_example dataset, w, obj, learning_rate, tol, max_iter, &block\n",
    "    iter = 1\n",
    "    until converged(last_loss, current_loss) do\n",
    "        w_last = w\n",
    "        loss = calc_loss(w)\n",
    "        w = update(w)\n",
    "        w = adjust(w)\n",
    "        iter += 1\n",
    "        break unless yield w, iter, loss\n",
    "    end\n",
    "    \n",
    "    return w\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "There are three main parts to the algorithm above:\n",
    "1. Convergence is based on the absolute difference between the loss of the current and previoud iteration.\n",
    "1. The norm can be calculated as the dot product of two vectors: $||w|| = w \\cdot w$\n",
    "1. Once the loss and gradient functions are calculated, we will update the values of each weight\n",
    "\n",
    "### Implement dot product\n",
    "In this first part, implement the dot product. The dot product below should be for sparse vectors. Use ```has_key?``` to skip entries in vector ```w``` not present in ```x```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cd831dd58d1fca904a54df797cbaf64b",
     "grade": false,
     "grade_id": "cell-7af25ba96bfb8ab2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":dot"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot x, w\n",
    "  sum=0\n",
    "  x.each do |key1, array1|\n",
    "    w.each do |key2, array2|\n",
    "      if key1==key2 then\n",
    "        sum+=array1*array2\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  return sum\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "729b2f04141c6e728d5a50c168864202",
     "grade": true,
     "grade_id": "cell-6e56a41a0ad960bf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0}), 1e-6\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0, \"b\" => 4.0}), 1e-6\n",
    "assert_equal 0.0, dot({}, {})\n",
    "assert_equal 0.0, dot({\"a\" => 1.0}, {\"b\" => 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 (1 Point)\n",
    "Implement the L2 norm for a vector, i.e., $\\left \\lVert x \\right \\rVert$, when represented by a hash. Hint: use ```dot``` and don't forget the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87e05e64c2e89f801ccf85a4b394f421",
     "grade": false,
     "grade_id": "cell-4b9adf1cdcc42b60",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":norm"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm w\n",
    "  return dot(w,w)**(0.5)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c875d2607666516fcc6e381176393f8",
     "grade": true,
     "grade_id": "cell-6732c7d27e4f664a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 2.0, norm({\"a\" => 1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 2.0, norm({\"a\" => -1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 0.0, norm({}), 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 (3 points)\n",
    "Implement a function that updates a weight vector, ```w```, given a gradient vector, ```dw```, and learning rate, ```lr```.  Do not change the original weight vector. Hint: use ```clone```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a759dd69710e353e41a8435530c516cd",
     "grade": false,
     "grade_id": "cell-7353e3fd009c70fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":update_weights"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_weights(w, dw, lr)\n",
    "  w2=w.clone\n",
    "  w2.each do |key1, array1|\n",
    "    dw.each do |key2, array2|\n",
    "      if key1==key2 then\n",
    "        w2[key1]=array1-dw[key2]*lr.to_f\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "    return w2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35238b8fa6c3aab46d461bb48957817e",
     "grade": true,
     "grade_id": "cell-7890d2e9cc768dbc",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 1.5, update_weights({\"a\" => 1.0}, {\"a\" => -0.25}, 2.0)[\"a\"], 1e-2\n",
    "assert_in_delta 2.5, update_weights({\"a\" => 1.0, \"b\" => 3.0}, {\"a\" => -0.25, \"b\" => 0.25}, 2.0)[\"b\"], 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (15 Points)\n",
    "\n",
    "Now, put all these functions together to implement gradient descent. This function takes the ```dataset``` and calls the function and gradient on all the examples. Hint: Increment ```iter``` before calling ```yield```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bd9bf72182964e413bb3974d81e358fe",
     "grade": false,
     "grade_id": "cell-54768dfb34dacd23",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":gradient_descent"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent dataset, w, obj, learning_rate, tol, max_iter, &block  \n",
    "  examples = dataset[\"data\"]\n",
    "  iter = 1\n",
    "  lr=learning_rate\n",
    "  loss_current=obj.func(dataset[\"data\"], w.clone)\n",
    "  \n",
    "  until (loss_current<tol or max_iter<iter) do\n",
    "    w_last = w.clone\n",
    "    grad = obj.grad(dataset[\"data\"], w.clone)\n",
    "    w = update_weights(w.clone, grad, lr)\n",
    "    w=obj.adjust(w)\n",
    "    loss_current = obj.func(dataset[\"data\"], w.clone)\n",
    "    break unless yield w, iter, loss_current\n",
    "    iter += 1\n",
    "  end\n",
    "  return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ec458221cdd59bb7bf14ab35906c4d4b",
     "grade": true,
     "grade_id": "cell-e54d5e8595d96ad4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t4 = ParabolaObjective.new\n",
    "t4_w_init = {\"0\" => 3.0, \"1\" => 7.0}\n",
    "t4_w_goal = {\"0\" => 1.0, \"1\" => 2.0}\n",
    "t4_dataset = {\"data\" => []}\n",
    "\n",
    "t4_loss = 1.0\n",
    "t4_w = nil\n",
    "gradient_descent t4_dataset, t4_w_init, t4, 0.1, 0.001, 100 do |w, iter, loss|\n",
    "  t4_loss = loss\n",
    "  t4_w = w\n",
    "end\n",
    "\n",
    "assert_in_delta 0.01, t4_loss, 1e-2\n",
    "assert_in_delta 1.0, t4_w[\"0\"], 1e-1\n",
    "assert_in_delta 2.0, t4_w[\"1\"], 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8815332ed062dfb1962b2cb26c92e51f",
     "grade": true,
     "grade_id": "cell-208aaf7c7bf7b770",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t4 = ParabolaObjective.new\n",
    "t4_w_init = {\"0\" => 3.0, \"1\" => 7.0}\n",
    "t4_data = {\"data\" => []}\n",
    "\n",
    "t4_total_loss = 0.0\n",
    "t4_iterations = []\n",
    "t4_losses = []\n",
    "gradient_descent t4_data, t4_w_init, t4, 0.1, 0.001, 100 do |w, iter, loss|\n",
    "  t4_total_loss += loss\n",
    "  t4_iterations << iter\n",
    "  assert_true(iter > 0, \"Make sure to increment 'iter' before calling 'yield'\")\n",
    "  t4_losses << t4_total_loss / iter\n",
    "end\n",
    "\n",
    "assert_true(t4_iterations.size > 30)\n",
    "assert_true(t4_losses[-1] < 3)\n",
    "assert_true(t4_losses[-1] > 0)\n",
    "assert_true(t4_iterations[-1] > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id='vis-a8b97d08-e437-4a88-9463-52e49fb2f546'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"6bad7bdd-290f-421f-8e53-7a4d307b4a9a\"}],\"options\":{\"x_label\":\"Iteration\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[1,46],\"yrange\":[1.3437385916027726,11.745]}}],\"data\":{\"6bad7bdd-290f-421f-8e53-7a4d307b4a9a\":[{\"x\":1,\"y\":11.745},{\"x\":2,\"y\":10.629224999999998},{\"x\":3,\"y\":9.654781499999999},{\"x\":4,\"y\":8.801529761249999},{\"x\":5,\"y\":8.052391285289998},{\"x\":6,\"y\":7.392864117570748},{\"x\":7,\"y\":6.810617087341976},{\"x\":8,\"y\":6.295149860653625},{\"x\":9,\"y\":5.83750789967061},{\"x\":10,\"y\":5.430043258859874},{\"x\":11,\"y\":5.06621367243318},{\"x\":12,\"y\":4.740413651781636},{\"x\":13,\"y\":4.4478323611782695},{\"x\":14,\"y\":4.184333911657656},{\"x\":15,\"y\":3.9463564372131876},{\"x\":16,\"y\":3.730826919508764},{\"x\":17,\"y\":3.5350892280490345},{\"x\":18,\"y\":3.356843259457511},{\"x\":19,\"y\":3.194093406467922},{\"x\":20,\"y\":3.045104876277066},{\"x\":21,\"y\":2.9083666188423076},{\"x\":22,\"y\":2.7825598266594387},{\"x\":23,\"y\":2.6665311352639653},{\"x\":24,\"y\":2.559269793748653},{\"x\":25,\"y\":2.4598881916189526},{\"x\":26,\"y\":2.3676052261647613},{\"x\":27,\"y\":2.2817320764085136},{\"x\":28,\"y\":2.201660018251936},{\"x\":29,\"y\":2.1268499728949624},{\"x\":30,\"y\":2.0568235287767553},{\"x\":31,\"y\":1.9911552177185536},{\"x\":32,\"y\":1.9294658599035275},{\"x\":33,\"y\":1.8714168208696798},{\"x\":34,\"y\":1.8167050477013689},{\"x\":35,\"y\":1.7650587718198771},{\"x\":36,\"y\":1.716233782808153},{\"x\":37,\"y\":1.6700101920725878},{\"x\":38,\"y\":1.6261896172740908},{\"x\":39,\"y\":1.584592728710167},{\"x\":40,\"y\":1.5450571074988546},{\"x\":41,\"y\":1.5074353727551923},{\"x\":42,\"y\":1.4715935411714272},{\"x\":43,\"y\":1.4374095876895803},{\"x\":44,\"y\":1.404772180437002},{\"x\":45,\"y\":1.3735795669061055},{\"x\":46,\"y\":1.3437385916027726}]},\"extension\":[]}\n",
       "        var id_name = '#vis-a8b97d08-e437-4a88-9463-52e49fb2f546';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x0000000002b08018 @properties={:diagrams=>[#<Nyaplot::Diagram:0x0000000002b26680 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"6bad7bdd-290f-421f-8e53-7a4d307b4a9a\"}, @xrange=[1, 46], @yrange=[1.3437385916027726, 11.745]>], :options=>{:x_label=>\"Iteration\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[1, 46], :yrange=>[1.3437385916027726, 11.745]}}>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t4_iterations, y: t4_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1 (5 Points)\n",
    "\n",
    "Let's learn the parameter of a Bernoulli distribution using the method of maximum likelihood. Consider the following dataset in which we are tossing a biased coin with probability $\\mu$ of returning a success (1). There is an analytical solution to find this parameter $\\mu$ given a dataset of successes and trials. Compute this analytical solution. \n",
    "\n",
    "Here the ```label``` field is either 0 or 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset format\n",
    "This is the format for the coin dataset. The format below will be used throughout most of the assignments. A dataset contains some extra details like the classes and names of features. The ```data``` entry is an array of examples containing ```features``` and a label. Notice that in this particular dataset, there aren't any \"features\", just a ```bias``` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classes\": {\n",
      "  },\n",
      "  \"features\": [\n",
      "    \"x\"\n",
      "  ],\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"features\": {\n",
      "        \"bias\": 1.0\n",
      "      },\n",
      "      \"label\": 1.0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "puts JSON.pretty_generate(coin_dataset(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "908d76a78e7e78ba9a7ab1f3beed242e",
     "grade": false,
     "grade_id": "cell-5fa56022cb1d93b4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q31_binomial_parameter"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coin_data = coin_dataset(1000)\n",
    "\n",
    "def q31_binomial_parameter(coin_data)\n",
    "  sum=0\n",
    "  total=0\n",
    "  coin_data[\"data\"].each do |item|\n",
    "    sum=sum+(item[\"label\"]).to_f\n",
    "    total=total+1\n",
    "  end\n",
    "  puts (sum.to_f)/(total.to_f)\n",
    "  return (sum.to_f)/(total.to_f)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c9b7b7e94293f333e7c9ac657d48493",
     "grade": true,
     "grade_id": "cell-00e4a8fc6cd8141e",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7691\n"
     ]
    }
   ],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t31_coin_data = coin_dataset(10000)\n",
    "assert_in_delta 0.77, q31_binomial_parameter(t31_coin_data), 5e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 (5 Points)\n",
    "\n",
    "Now, let's use the maximum likelihood function and gradient descent to find the same parameter value. Define the objective function for a binomial distribution for multiple samples. Remember that the ```label``` is the target value and every example has only one feature, ```bias```. Learn the weight for the ```bias``` feature should converge to $w_{bias} = \\mu$.\n",
    "\n",
    "In this first step, calculate the log likelihood function for the binomial distribution of $n$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "537973344d496a0626033100a3d0060f",
     "grade": false,
     "grade_id": "cell-92354ef065795ab6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":func"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinomialModel\n",
    "  def func examples, w\n",
    "    count=0\n",
    "    datasize=examples.size\n",
    "    for i in 0..(datasize-1) do\n",
    "      if examples[i][\"label\"]==1 then\n",
    "        count+=1\n",
    "      end\n",
    "    end\n",
    "    p=w[\"bias\"]\n",
    "    return ((-count)*Math.log(p)-(datasize-count)*Math.log(1-p))\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b7eb211a6b8ebedb4c3219fe59cf8172",
     "grade": true,
     "grade_id": "cell-68cf9b7f4c38b214",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_model = BinomialModel.new\n",
    "t32_t1 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 0.0}\n",
    "t32_t2 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 1.0}\n",
    "t32_dataset = {\n",
    "  \"data\" => [t32_t1, t32_t2]\n",
    "}\n",
    "\n",
    "assert_in_delta 1.469677, t32_model.func([t32_t1], {\"bias\" => 0.77}), 1e-3\n",
    "assert_in_delta 0.261365, t32_model.func([t32_t2], {\"bias\" => 0.77}), 1e-3\n",
    "assert_in_delta 1.731041, t32_model.func(t32_dataset[\"data\"], {\"bias\" => 0.77}), 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 (5 Points)\n",
    "\n",
    "Calculate the gradient of the binomial log likelihood function over $n$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2d49d810b64d6a0a1fd9c006508bed0d",
     "grade": false,
     "grade_id": "cell-af9a1550d76082ae",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinomialModel\n",
    "  def grad examples, w\n",
    "    g={}\n",
    "    p=w[\"bias\"]\n",
    "    count=0\n",
    "    datasize=examples.size\n",
    "    for i in 0..(datasize-1) do\n",
    "      if examples[i][\"label\"]==1 then\n",
    "        count+=1\n",
    "      end\n",
    "    end\n",
    "    g[\"bias\"]=((-count)/(p)+(datasize-count)*(1/(1-p)))\n",
    "    return g\n",
    "  end\n",
    "    def adjust w\n",
    "    w[\"bias\"] = [[0.001, w[\"bias\"]].max, 0.999].min\n",
    "    return w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc9f101bb22d328e59943e09873508f6",
     "grade": true,
     "grade_id": "cell-d87514aa1ef00351",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t33_model = BinomialModel.new\n",
    "t33_t1 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 0.0}\n",
    "t33_t2 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 1.0}\n",
    "\n",
    "t33_dataset = {\"data\" => [t33_t1, t33_t2]}\n",
    "\n",
    "assert_in_delta 4.347826, t32_model.grad([t33_t1], {\"bias\" => 0.77})[\"bias\"], 1e-3\n",
    "assert_in_delta -1.29870, t32_model.grad([t33_t2], {\"bias\" => 0.77})[\"bias\"], 1e-3\n",
    "assert_in_delta 3.049124, t32_model.grad(t33_dataset[\"data\"], {\"bias\" => 0.77})[\"bias\"], 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.4 (5 Points)\n",
    "\n",
    "Putting the objective function to work, use gradient descent to find the parameter for the binomial distribution. You get to set the learning rate. Return the learning rate you have obtained which works well. You may have to try a few until you get it right.\n",
    "\n",
    "Note that, while capable of returning the same value, this method reads the data many more times than the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the learning rate\n",
    "Here, set this function to return one number. For example, if you decide that the learning rate needs to be 1.234, implement the following. Note: 1.234 might not be the best choice.\n",
    "\n",
    "```ruby\n",
    "def q34_learning_rate\n",
    "  1.234\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9afa2f41321b339e15c344f2cc4070e5",
     "grade": false,
     "grade_id": "cell-ca57b6ffd6468fe0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q34_learning_rate"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q34_learning_rate\n",
    "  return 0.00001\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "350ee4c1ef4303ce3a95adc0bd015575",
     "grade": true,
     "grade_id": "cell-03c7000ad0157d31",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{\"bias\"=>0.5866049508927114}\t\n",
      "11\t{\"bias\"=>0.7804134998199271}\t5391.81340478645\n",
      "21\t{\"bias\"=>0.7804999864688038}\t5327.27871264938\n",
      "31\t{\"bias\"=>0.7804999999978848}\t5305.767139798877\n",
      "41\t{\"bias\"=>0.7804999999999996}\t5295.011353373624\n",
      "51\t{\"bias\"=>0.7805}\t5288.557881518474\n",
      "61\t{\"bias\"=>0.7805}\t5284.255566948372\n",
      "71\t{\"bias\"=>0.7805}\t5281.182485112586\n",
      "81\t{\"bias\"=>0.7805}\t5278.877673735746\n",
      "91\t{\"bias\"=>0.7805}\t5277.085042664869\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-1078d73e-4539-4ece-a664-1d120c9276a3'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"33a0630e-2e93-4bf9-9d1e-0be6a362d33c\"}],\"options\":{\"x_label\":\"Iteration\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[1,100],\"yrange\":[5275.65093780817,6102.172521073473]}}],\"data\":{\"33a0630e-2e93-4bf9-9d1e-0be6a362d33c\":[{\"x\":1,\"y\":6102.172521073473},{\"x\":2,\"y\":5839.409498860663},{\"x\":3,\"y\":5681.192909853687},{\"x\":4,\"y\":5583.473577205132},{\"x\":5,\"y\":5520.577592915011},{\"x\":6,\"y\":5477.812982712779},{\"x\":7,\"y\":5447.121780350796},{\"x\":8,\"y\":5424.079698288362},{\"x\":9,\"y\":5406.1543107411535},{\"x\":10,\"y\":5391.81340478645},{\"x\":11,\"y\":5380.079841844816},{\"x\":12,\"y\":5370.301857688525},{\"x\":13,\"y\":5362.028176378912},{\"x\":14,\"y\":5354.936449154411},{\"x\":15,\"y\":5348.790285497124},{\"x\":16,\"y\":5343.412392286808},{\"x\":17,\"y\":5338.66719239369},{\"x\":18,\"y\":5334.449236932869},{\"x\":19,\"y\":5330.675276783668},{\"x\":20,\"y\":5327.27871264938},{\"x\":21,\"y\":5324.205630813594},{\"x\":22,\"y\":5321.411920053789},{\"x\":23,\"y\":5318.861140664402},{\"x\":24,\"y\":5316.52292622413},{\"x\":25,\"y\":5314.37176893908},{\"x\":26,\"y\":5312.38608529134},{\"x\":27,\"y\":5310.547489321212},{\"x\":28,\"y\":5308.840221634664},{\"x\":29,\"y\":5307.2506965471875},{\"x\":30,\"y\":5305.767139798877},{\"x\":31,\"y\":5304.379296389167},{\"x\":32,\"y\":5303.078193192564},{\"x\":33,\"y\":5301.855944735149},{\"x\":34,\"y\":5300.705593245817},{\"x\":35,\"y\":5299.620976127304},{\"x\":36,\"y\":5298.596615515376},{\"x\":37,\"y\":5297.627625747335},{\"x\":38,\"y\":5296.7096354407695},{\"x\":39,\"y\":5295.838721560182},{\"x\":40,\"y\":5295.011353373624},{\"x\":41,\"y\":5294.2243446108005},{\"x\":42,\"y\":5293.474812455731},{\"x\":43,\"y\":5292.7601422613625},{\"x\":44,\"y\":5292.077957075828},{\"x\":45,\"y\":5291.426091231873},{\"x\":46,\"y\":5290.802567381134},{\"x\":47,\"y\":5290.205576460214},{\"x\":48,\"y\":5289.633460160998},{\"x\":49,\"y\":5289.084695547464},{\"x\":50,\"y\":5288.557881518474},{\"x\":51,\"y\":5288.0517268631675},{\"x\":52,\"y\":5287.565039694604},{\"x\":53,\"y\":5287.096718079571},{\"x\":54,\"y\":5286.645741709539},{\"x\":55,\"y\":5286.211164480236},{\"x\":56,\"y\":5285.792107866265},{\"x\":57,\"y\":5285.387754993136},{\"x\":58,\"y\":5284.9973453225275},{\"x\":59,\"y\":5284.620169878041},{\"x\":60,\"y\":5284.255566948372},{\"x\":61,\"y\":5283.902918213118},{\"x\":62,\"y\":5283.561645243517},{\"x\":63,\"y\":5283.231206336443},{\"x\":64,\"y\":5282.911093645215},{\"x\":65,\"y\":5282.600830575256},{\"x\":66,\"y\":5282.299969416507},{\"x\":67,\"y\":5282.0080891878715},{\"x\":68,\"y\":5281.724793671841},{\"x\":69,\"y\":5281.449709620045},{\"x\":70,\"y\":5281.182485112586},{\"x\":71,\"y\":5280.92278805604},{\"x\":72,\"y\":5280.670304806621},{\"x\":73,\"y\":5280.424738906501},{\"x\":74,\"y\":5280.1858099226},{\"x\":75,\"y\":5279.953252378271},{\"x\":76,\"y\":5279.726814769318},{\"x\":77,\"y\":5279.506258656702},{\"x\":78,\"y\":5279.2913578290245},{\"x\":79,\"y\":5279.08189752863},{\"x\":80,\"y\":5278.877673735746},{\"x\":81,\"y\":5278.678492505648},{\"x\":82,\"y\":5278.484169354334},{\"x\":83,\"y\":5278.294528688593},{\"x\":84,\"y\":5278.109403276799},{\"x\":85,\"y\":5277.928633757047},{\"x\":86,\"y\":5277.752068179614},{\"x\":87,\"y\":5277.579561580974},{\"x\":88,\"y\":5277.410975586848},{\"x\":89,\"y\":5277.246178042027},{\"x\":90,\"y\":5277.085042664869},{\"x\":91,\"y\":5276.927448724573},{\"x\":92,\"y\":5276.7732807395005},{\"x\":93,\"y\":5276.622428194966},{\"x\":94,\"y\":5276.47478527904},{\"x\":95,\"y\":5276.330250635027},{\"x\":96,\"y\":5276.188727129432},{\"x\":97,\"y\":5276.050121634261},{\"x\":98,\"y\":5275.914344822665},{\"x\":99,\"y\":5275.78131097696},{\"x\":100,\"y\":5275.65093780817}]},\"extension\":[]}\n",
       "        var id_name = '#vis-1078d73e-4539-4ece-a664-1d120c9276a3';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x0000000002b9dca8 @properties={:diagrams=>[#<Nyaplot::Diagram:0x00000000029e9808 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"33a0630e-2e93-4bf9-9d1e-0be6a362d33c\"}, @xrange=[1, 100], @yrange=[5275.65093780817, 6102.172521073473]>], :options=>{:x_label=>\"Iteration\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[1, 100], :yrange=>[5275.65093780817, 6102.172521073473]}}>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t34 = BinomialModel.new\n",
    "t34_w_init = {\"bias\" => rand}\n",
    "t34_data = coin_dataset(10000)\n",
    "\n",
    "t34_learning_rate = q34_learning_rate()\n",
    "\n",
    "t34_total_loss = 0.0\n",
    "t34_iterations = []\n",
    "t34_losses = []\n",
    "t34_w = t34_w_init\n",
    "gradient_descent t34_data, t34_w_init, t34, t34_learning_rate, 0.001, 100 do |w, iter, loss|\n",
    "  puts [iter, w, t34_losses[-1]].join(\"\\t\") if iter % 10 == 1\n",
    "  t34_total_loss += loss\n",
    "  t34_iterations << iter\n",
    "  assert_true(iter > 0, \"Make sure to increment iter before calling yield\")\n",
    "  t34_losses << t34_total_loss / iter.to_f\n",
    "  t34_w = w\n",
    "end\n",
    "\n",
    "\n",
    "assert_true(t34_losses[-1] < 8000)\n",
    "assert_true(t34_losses[-1] > 0)\n",
    "assert_true(t34_iterations[-1] > 30)\n",
    "assert_in_delta 0.77, t34_w[\"bias\"], 5e-2\n",
    "\n",
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t34_iterations, y: t34_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (10 Points)\n",
    "\n",
    "The maximum likelihood method above reads the data multiple times and can benefit from prior knowledge in the form of a prior distribution for the parameter, $\\mu$. Using the Beta distribution as the conjugate prior, implement the likelihood function and its gradient. Now we are learning three parameters altogether: $w_{bias} = \\mu$, $\\alpha$, $\\beta$.\n",
    "\n",
    "First, let's compute the closed form estimator for $\\mu$ with a fixed $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7fe610db1d60504b6baacd8c486dcfde",
     "grade": false,
     "grade_id": "cell-97d4bb1bc5d5a0f3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q41_closed_form_beta_binomial"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q41_closed_form_beta_binomial(coin_data, alpha, beta)\n",
    "  return (alpha-1)/(alpha+beta-2).to_f\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "af6ab15034ba413fb34cf9c99f3f715f",
     "grade": true,
     "grade_id": "cell-e089341132a19f54",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t41_coin_data = coin_dataset(10000)\n",
    "assert_in_delta 0.77, q41_closed_form_beta_binomial(t41_coin_data, 7743, 10000 - 7743), 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2 (10 points)\n",
    "\n",
    "Implement the negative log likelihood function for the beta + binomial values. Checkout this [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Maximum_likelihood) definition of the likelihood function. Remember we are interested in he negative log likelihood.\n",
    "\n",
    "A special function is needed ```GSL::Sf::lnbeta(a, b)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ae3b18deff351186867bca1e042c1992",
     "grade": false,
     "grade_id": "cell-0c423e99f1c034e0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BetaBinomialModel\n",
    "  def func dataset, w\n",
    "    sum1=0.0\n",
    "    sum2=0.0\n",
    "    alpha=w[\"alpha\"]\n",
    "    beta=w[\"beta\"]\n",
    "    adjust(w)\n",
    "    p=w[\"bias\"]\n",
    "    size=dataset.size\n",
    "    count=0\n",
    "    \n",
    "    for i in 0..(size-1) do\n",
    "      if dataset[i][\"label\"]==1.0\n",
    "        count+=1\n",
    "      end\n",
    "    end\n",
    "    \n",
    "    for i in 0..(size-1) do\n",
    "      sum1=sum1+Math.log(p)\n",
    "    end\n",
    "    \n",
    "    for i in 0..(size-1) do\n",
    "      sum2=sum2+Math.log((1-p))\n",
    "    end\n",
    "    \n",
    "    a = ((alpha-1)*(size)+count)*Math.log(p)\n",
    "    b = ((beta-1)*(size)+size-count)*Math.log(1-p)\n",
    "    c = -1*GSL::Sf::lnbeta(alpha, beta)*(size)\n",
    "\n",
    "    answer=a+b+c\n",
    "\n",
    "    return -answer\n",
    "  end\n",
    "  \n",
    "  def adjust w\n",
    "    w[\"bias\"] = [[0.001, w[\"bias\"]].max, 0.999].min\n",
    "    w[\"beta\"] = [0.0001, w[\"beta\"]].max\n",
    "    return w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18a9751dfa872fc62f990d54afd87b64",
     "grade": true,
     "grade_id": "cell-c3da75938a85ae1b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t42 = BetaBinomialModel.new\n",
    "srand 777 #seed random number generator\n",
    "t42_data = coin_dataset(1000)[\"data\"]\n",
    "t42_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "t42_w[\"alpha\"] = 7.0\n",
    "t42_w[\"beta\"] = 3.0\n",
    "\n",
    "assert_in_delta 10373.126026759332, t42.func(t42_data, t42_w), 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3 (10 points)\n",
    "\n",
    "Implement the negative log likelihood gradient for all the parameters. Checkout this [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Maximum_likelihood) definition of the likelihood function. Remember we are interested in the negative log likelihood. The gradient for the ```bias``` requires all examples in the dataset. However, the gradient for ```alpha``` and ```beta``` does not require a pass over the dataset.\n",
    "\n",
    "A special function is needed ```GSL::Sf::psi(a + b)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "17bd13df38d687cdb957a67a4a762db0",
     "grade": false,
     "grade_id": "cell-9d4f8012ea73cfa8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BetaBinomialModel\n",
    "  def grad dataset, w\n",
    "    sum1=0.0\n",
    "    sum2=0.0\n",
    "    alpha=w[\"alpha\"]\n",
    "    beta=w[\"beta\"]\n",
    "    p=w[\"bias\"]\n",
    "    size=dataset.size\n",
    "    grad={}\n",
    "\n",
    "    count=0\n",
    "\n",
    "    for i in 0..(size-1) do\n",
    "      if dataset[i][\"label\"]==1.0\n",
    "        count+=1\n",
    "      end\n",
    "    end\n",
    "\n",
    "    a=((alpha-1)+count)/p\n",
    "    b=((beta-1)+size-count)/(1-p)\n",
    "    grad[\"bias\"]=-a+b\n",
    "    \n",
    "    grad[\"alpha\"]=-(Math.log(p)-(-GSL::Sf::psi(alpha + beta)+GSL::Sf::psi(alpha)))\n",
    "    \n",
    "    grad[\"beta\"]=-(Math.log(1-p)-(-GSL::Sf::psi(alpha + beta)+GSL::Sf::psi(beta)))\n",
    "\n",
    "    return grad\n",
    "\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25a6d3730d8f7b4dc750dbcb27184354",
     "grade": true,
     "grade_id": "cell-2752de436c18d1fd",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t43 = BetaBinomialModel.new\n",
    "srand 777 #seed random number generator\n",
    "t43_data = coin_dataset(1000)[\"data\"]\n",
    "t43_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "t43_w[\"alpha\"] = 7.0\n",
    "t43_w[\"beta\"] = 3.0\n",
    "\n",
    "t43_grad = t43.grad(t43_data, t43_w)\n",
    "\n",
    "assert_in_delta -7902.2222222221935, t43_grad[\"bias\"], 1e2\n",
    "assert_in_delta 1.9236168390257913, t43_grad[\"alpha\"], 1e-1\n",
    "assert_in_delta -1.2236077383104214, t43_grad[\"beta\"], 1e-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.4 (20 points)\n",
    "\n",
    "Run the gradient descent by selecting the initial weights and learning rate. Try a few values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "706ade57f59434796197e6bd977d6a95",
     "grade": false,
     "grade_id": "cell-963091d78ac84fe4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q44_weights_and_learning_rate"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q44_weights_and_learning_rate\n",
    "  w={}\n",
    "  w[\"alpha\"]=0.7\n",
    "  w[\"beta\"]=0.455\n",
    "  w[\"bias\"]=0.77\n",
    "  lr=0.000001\n",
    "  return [w, lr]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{\"alpha\"=>0.7000006110393128, \"beta\"=>0.45500038694404443, \"bias\"=>0.7704367625635234}\t\n",
      "11\t{\"alpha\"=>0.7000067479083389, \"beta\"=>0.45500416699663254, \"bias\"=>0.7736692085416402}\t6443.590568736936\n",
      "21\t{\"alpha\"=>0.7000129174250792, \"beta\"=>0.45500783552710217, \"bias\"=>0.7754808824683709}\t6417.776278484134\n",
      "31\t{\"alpha\"=>0.7000191051119424, \"beta\"=>0.45501144116569014, \"bias\"=>0.7764888258471395}\t6399.497710137873\n",
      "41\t{\"alpha\"=>0.7000253028539046, \"beta\"=>0.45501501167041286, \"bias\"=>0.7770472549621588}\t6386.311325335448\n",
      "51\t{\"alpha\"=>0.7000315061358376, \"beta\"=>0.45501856264284557, \"bias\"=>0.7773559105125312}\t6376.599992111794\n",
      "61\t{\"alpha\"=>0.7000377124561584, \"beta\"=>0.45502210277568567, \"bias\"=>0.7775262864753067}\t6369.292815112673\n",
      "71\t{\"alpha\"=>0.7000439204321666, \"beta\"=>0.455025636888613, \"bias\"=>0.7776202642974038}\t6363.675958845294\n",
      "81\t{\"alpha\"=>0.7000501293006134, \"beta\"=>0.4550291676466597, \"bias\"=>0.7776720806536951}\t6359.268614408228\n",
      "91\t{\"alpha\"=>0.7000563386404962, \"beta\"=>0.4550326965212359, \"bias\"=>0.7777006441351357}\t6355.742707767341\n",
      "it's one\n",
      "[6476.25921835393, 6472.074685125584, 6468.0416637728495, 6464.1543490785025, 6460.407142117909, 6456.794645316934, 6453.311657377841, 6449.953168104037, 6446.714353152083, 6443.590568736936, 6440.577346313986, 6437.670387259281, 6434.865557567082, 6432.1588825818735, 6429.546541780029, 6427.024863614503, 6424.590320434233, 6422.239523488348, 6419.969218023856, 6417.776278484134, 6415.657703814277, 6413.610612878326, 6411.632239992304, 6409.719930576113, 6407.871136926493, 6406.083414112518, 6404.354415994434, 6402.6818913660545, 6401.063680220407, 6399.497710137873, 6397.981992795653, 6396.514620597066, 6395.0937634188585, 6393.717665474479, 6392.384642291045, 6391.093077797543, 6389.841421521682, 6388.628185892679, 6387.451943647177, 6386.311325335448, 6385.205016924923, 6384.131757498176, 6383.090337042339, 6382.079594327061, 6381.098414868031, 6380.145728973181, 6379.22050986869, 6378.321771901952, 6377.448568818735, 6376.599992111794, 6375.775169438263, 6374.9732631032575, 6374.193468607084, 6373.435013253648, 6372.697154817623, 6371.9791802680475, 6371.280404546113, 6370.600169394936, 6369.937842239209, 6369.292815112673, 6368.664503631453, 6368.052346011338, 6367.455802127189, 6366.874352612676, 6366.307497998685, 6365.754757888709, 6365.215670169689, 6364.689790256772, 6364.1766903705375, 6363.675958845294, 6363.1871994671155, 6362.710030840318, 6362.244085781163, 6361.78901073759, 6361.3444652338585, 6360.910121339005, 6360.485663158083, 6360.070786345195, 6359.665197637345, 6359.268614408228, 6358.880764241063, 6358.501384519638, 6358.130222036781, 6357.767032619471, 6357.41158076989, 6357.063639321678, 6356.722989110759, 6356.389418660081, 6356.062723877658, 6355.742707767341, 6355.42918015176, 6355.121957406879, 6354.820862207701, 6354.525723284591, 6354.236375189778, 6353.952658073584, 6353.674417469952, 6353.401504090878, 6353.133773629349, 6352.871086570417]\n",
      "it's two\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-a2d5bba3-90ef-430f-a0de-e58b0f2110c5'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"4807ae74-b489-45e8-b1aa-184e25e2de69\"}],\"options\":{\"x_label\":\"Iteration\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[1,100],\"yrange\":[6352.871086570417,6476.25921835393]}}],\"data\":{\"4807ae74-b489-45e8-b1aa-184e25e2de69\":[{\"x\":1,\"y\":6476.25921835393},{\"x\":2,\"y\":6472.074685125584},{\"x\":3,\"y\":6468.0416637728495},{\"x\":4,\"y\":6464.1543490785025},{\"x\":5,\"y\":6460.407142117909},{\"x\":6,\"y\":6456.794645316934},{\"x\":7,\"y\":6453.311657377841},{\"x\":8,\"y\":6449.953168104037},{\"x\":9,\"y\":6446.714353152083},{\"x\":10,\"y\":6443.590568736936},{\"x\":11,\"y\":6440.577346313986},{\"x\":12,\"y\":6437.670387259281},{\"x\":13,\"y\":6434.865557567082},{\"x\":14,\"y\":6432.1588825818735},{\"x\":15,\"y\":6429.546541780029},{\"x\":16,\"y\":6427.024863614503},{\"x\":17,\"y\":6424.590320434233},{\"x\":18,\"y\":6422.239523488348},{\"x\":19,\"y\":6419.969218023856},{\"x\":20,\"y\":6417.776278484134},{\"x\":21,\"y\":6415.657703814277},{\"x\":22,\"y\":6413.610612878326},{\"x\":23,\"y\":6411.632239992304},{\"x\":24,\"y\":6409.719930576113},{\"x\":25,\"y\":6407.871136926493},{\"x\":26,\"y\":6406.083414112518},{\"x\":27,\"y\":6404.354415994434},{\"x\":28,\"y\":6402.6818913660545},{\"x\":29,\"y\":6401.063680220407},{\"x\":30,\"y\":6399.497710137873},{\"x\":31,\"y\":6397.981992795653},{\"x\":32,\"y\":6396.514620597066},{\"x\":33,\"y\":6395.0937634188585},{\"x\":34,\"y\":6393.717665474479},{\"x\":35,\"y\":6392.384642291045},{\"x\":36,\"y\":6391.093077797543},{\"x\":37,\"y\":6389.841421521682},{\"x\":38,\"y\":6388.628185892679},{\"x\":39,\"y\":6387.451943647177},{\"x\":40,\"y\":6386.311325335448},{\"x\":41,\"y\":6385.205016924923},{\"x\":42,\"y\":6384.131757498176},{\"x\":43,\"y\":6383.090337042339},{\"x\":44,\"y\":6382.079594327061},{\"x\":45,\"y\":6381.098414868031},{\"x\":46,\"y\":6380.145728973181},{\"x\":47,\"y\":6379.22050986869},{\"x\":48,\"y\":6378.321771901952},{\"x\":49,\"y\":6377.448568818735},{\"x\":50,\"y\":6376.599992111794},{\"x\":51,\"y\":6375.775169438263},{\"x\":52,\"y\":6374.9732631032575},{\"x\":53,\"y\":6374.193468607084},{\"x\":54,\"y\":6373.435013253648},{\"x\":55,\"y\":6372.697154817623},{\"x\":56,\"y\":6371.9791802680475},{\"x\":57,\"y\":6371.280404546113},{\"x\":58,\"y\":6370.600169394936},{\"x\":59,\"y\":6369.937842239209},{\"x\":60,\"y\":6369.292815112673},{\"x\":61,\"y\":6368.664503631453},{\"x\":62,\"y\":6368.052346011338},{\"x\":63,\"y\":6367.455802127189},{\"x\":64,\"y\":6366.874352612676},{\"x\":65,\"y\":6366.307497998685},{\"x\":66,\"y\":6365.754757888709},{\"x\":67,\"y\":6365.215670169689},{\"x\":68,\"y\":6364.689790256772},{\"x\":69,\"y\":6364.1766903705375},{\"x\":70,\"y\":6363.675958845294},{\"x\":71,\"y\":6363.1871994671155},{\"x\":72,\"y\":6362.710030840318},{\"x\":73,\"y\":6362.244085781163},{\"x\":74,\"y\":6361.78901073759},{\"x\":75,\"y\":6361.3444652338585},{\"x\":76,\"y\":6360.910121339005},{\"x\":77,\"y\":6360.485663158083},{\"x\":78,\"y\":6360.070786345195},{\"x\":79,\"y\":6359.665197637345},{\"x\":80,\"y\":6359.268614408228},{\"x\":81,\"y\":6358.880764241063},{\"x\":82,\"y\":6358.501384519638},{\"x\":83,\"y\":6358.130222036781},{\"x\":84,\"y\":6357.767032619471},{\"x\":85,\"y\":6357.41158076989},{\"x\":86,\"y\":6357.063639321678},{\"x\":87,\"y\":6356.722989110759},{\"x\":88,\"y\":6356.389418660081},{\"x\":89,\"y\":6356.062723877658},{\"x\":90,\"y\":6355.742707767341},{\"x\":91,\"y\":6355.42918015176},{\"x\":92,\"y\":6355.121957406879},{\"x\":93,\"y\":6354.820862207701},{\"x\":94,\"y\":6354.525723284591},{\"x\":95,\"y\":6354.236375189778},{\"x\":96,\"y\":6353.952658073584},{\"x\":97,\"y\":6353.674417469952},{\"x\":98,\"y\":6353.401504090878},{\"x\":99,\"y\":6353.133773629349},{\"x\":100,\"y\":6352.871086570417}]},\"extension\":[]}\n",
       "        var id_name = '#vis-a2d5bba3-90ef-430f-a0de-e58b0f2110c5';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x00000000031decc0 @properties={:diagrams=>[#<Nyaplot::Diagram:0x00000000031a5808 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"4807ae74-b489-45e8-b1aa-184e25e2de69\"}, @xrange=[1, 100], @yrange=[6352.871086570417, 6476.25921835393]>], :options=>{:x_label=>\"Iteration\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[1, 100], :yrange=>[6352.871086570417, 6476.25921835393]}}>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t44 = BetaBinomialModel.new\n",
    "t44_data = coin_dataset(10000)\n",
    "\n",
    "t44_w_init, t44_learning_rate = q44_weights_and_learning_rate()\n",
    "\n",
    "t44_total_loss = 0.0\n",
    "t44_iterations = []\n",
    "t44_losses = []\n",
    "t44_w = t34_w_init\n",
    "gradient_descent t44_data, t44_w_init, t44, t44_learning_rate, 0.001, 100 do |w, iter, loss|\n",
    "  puts [iter, w, t44_losses[-1]].join(\"\\t\") if iter % 10 == 1\n",
    "  t44_total_loss += loss\n",
    "  t44_iterations << iter\n",
    "  t44_losses << t44_total_loss / iter.to_f\n",
    "  t44_w = w\n",
    "end\n",
    "\n",
    "\n",
    "puts \"it's one\"\n",
    "puts t44_losses\n",
    "assert_true(t44_losses[-1] < 8000)\n",
    "puts \"it's two\"\n",
    "puts t44_iterations\n",
    "assert_true(t44_iterations[-1] > 30)\n",
    "assert_in_delta 0.77, t44_w[\"bias\"], 5e-2\n",
    "\n",
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t44_iterations, y: t44_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "445d5c9fb43b646e7d49fd0774b9c231",
     "grade": true,
     "grade_id": "cell-2bf436344900b4d6",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{\"alpha\"=>0.7000006110393128, \"beta\"=>0.45500038694404443, \"bias\"=>0.7704762882552231}\t\n",
      "11\t{\"alpha\"=>0.7000067503096319, \"beta\"=>0.4550041588379397, \"bias\"=>0.7740028014066637}\t6431.263958176182\n",
      "21\t{\"alpha\"=>0.7000129251934665, \"beta\"=>0.45500780889791337, \"bias\"=>0.7759796975963831}\t6403.022259586012\n",
      "31\t{\"alpha\"=>0.7000191198929929, \"beta\"=>0.45501139020421055, \"bias\"=>0.7770790362947435}\t6383.021195344271\n",
      "41\t{\"alpha\"=>0.7000253255511569, \"beta\"=>0.4550149331130293, \"bias\"=>0.7776875645421539}\t6368.593604862423\n",
      "51\t{\"alpha\"=>0.7000315372422828, \"beta\"=>0.4550184546943021, \"bias\"=>0.778023538443271}\t6357.970641045892\n",
      "61\t{\"alpha\"=>0.7000377522397137, \"beta\"=>0.4550219644554369, \"bias\"=>0.7782087652458693}\t6349.979780118316\n",
      "71\t{\"alpha\"=>0.7000439690380085, \"beta\"=>0.45502546766349655, \"bias\"=>0.7783108015906552}\t6343.839170155395\n",
      "81\t{\"alpha\"=>0.70005018680719, \"beta\"=>0.4550289672277521, \"bias\"=>0.7783669857646873}\t6339.022190797245\n",
      "91\t{\"alpha\"=>0.7000564050900435, \"beta\"=>0.4550324647525055, \"bias\"=>0.7783979148435068}\t6335.169575012093\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-cdafbc03-1f8f-4335-9809-bff552bcb598'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"489ded59-5088-4d49-86d4-b78ae5399634\"}],\"options\":{\"x_label\":\"Iteration\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[1,100],\"yrange\":[6332.032638275235,6466.981467387201]}}],\"data\":{\"489ded59-5088-4d49-86d4-b78ae5399634\":[{\"x\":1,\"y\":6466.981467387201},{\"x\":2,\"y\":6462.4083197927175},{\"x\":3,\"y\":6458.000173718382},{\"x\":4,\"y\":6453.750767818448},{\"x\":5,\"y\":6449.65405756061},{\"x\":6,\"y\":6445.704210577341},{\"x\":7,\"y\":6441.895601819889},{\"x\":8,\"y\":6438.2228085508905},{\"x\":9,\"y\":6434.680605208973},{\"x\":10,\"y\":6431.263958176182},{\"x\":11,\"y\":6427.968020476538},{\"x\":12,\"y\":6424.7881264315465},{\"x\":13,\"y\":6421.719786296137},{\"x\":14,\"y\":6418.758680896196},{\"x\":15,\"y\":6415.900656286706},{\"x\":16,\"y\":6413.141718447417},{\"x\":17,\"y\":6410.478028031073},{\"x\":18,\"y\":6407.905895177385},{\"x\":19,\"y\":6405.421774404228},{\"x\":20,\"y\":6403.022259586012},{\"x\":21,\"y\":6400.704079027708},{\"x\":22,\"y\":6398.46409064166},{\"x\":23,\"y\":6396.299277233144},{\"x\":24,\"y\":6394.206741899475},{\"x\":25,\"y\":6392.183703546491},{\"x\":26,\"y\":6390.2274925253},{\"x\":27,\"y\":6388.335546391392},{\"x\":28,\"y\":6386.505405787468},{\"x\":29,\"y\":6384.734710450657},{\"x\":30,\"y\":6383.021195344271},{\"x\":31,\"y\":6381.362686913662},{\"x\":32,\"y\":6379.757099465378},{\"x\":33,\"y\":6378.202431668333},{\"x\":34,\"y\":6376.696763175449},{\"x\":35,\"y\":6375.2382513638795},{\"x\":36,\"y\":6373.8251281917},{\"x\":37,\"y\":6372.455697168744},{\"x\":38,\"y\":6371.128330439094},{\"x\":39,\"y\":6369.841465972558},{\"x\":40,\"y\":6368.593604862423},{\"x\":41,\"y\":6367.383308726628},{\"x\":42,\"y\":6366.209197209445},{\"x\":43,\"y\":6365.069945580759},{\"x\":44,\"y\":6363.964282429954},{\"x\":45,\"y\":6362.890987451449},{\"x\":46,\"y\":6361.8488893189},{\"x\":47,\"y\":6360.836863645129},{\"x\":48,\"y\":6359.853831024844},{\"x\":49,\"y\":6358.898755157281},{\"x\":50,\"y\":6357.970641045892},{\"x\":51,\"y\":6357.0685332723215},{\"x\":52,\"y\":6356.191514341912},{\"x\":53,\"y\":6355.338703098065},{\"x\":54,\"y\":6354.509253202851},{\"x\":55,\"y\":6353.702351681323},{\"x\":56,\"y\":6352.917217527039},{\"x\":57,\"y\":6352.1531003664195},{\"x\":58,\"y\":6351.4092791795665},{\"x\":59,\"y\":6350.685061075317},{\"x\":60,\"y\":6349.979780118316},{\"x\":61,\"y\":6349.292796205998},{\"x\":62,\"y\":6348.6234939934375},{\"x\":63,\"y\":6347.971281864085},{\"x\":64,\"y\":6347.335590944483},{\"x\":65,\"y\":6346.715874161133},{\"x\":66,\"y\":6346.111605337733},{\"x\":67,\"y\":6345.522278331097},{\"x\":68,\"y\":6344.947406204109},{\"x\":69,\"y\":6344.3865204341355},{\"x\":70,\"y\":6343.839170155395},{\"x\":71,\"y\":6343.304921433818},{\"x\":72,\"y\":6342.7833565730125},{\"x\":73,\"y\":6342.274073449986},{\"x\":74,\"y\":6341.776684879351},{\"x\":75,\"y\":6341.290818004773},{\"x\":76,\"y\":6340.816113716486},{\"x\":77,\"y\":6340.352226093734},{\"x\":78,\"y\":6339.898821871073},{\"x\":79,\"y\":6339.455579927474},{\"x\":80,\"y\":6339.022190797245},{\"x\":81,\"y\":6338.598356201816},{\"x\":82,\"y\":6338.183788601482},{\"x\":83,\"y\":6337.778210766211},{\"x\":84,\"y\":6337.381355364717},{\"x\":85,\"y\":6336.9929645709635},{\"x\":86,\"y\":6336.6127896873595},{\"x\":87,\"y\":6336.240590783901},{\"x\":88,\"y\":6335.876136352576},{\"x\":89,\"y\":6335.51920297635},{\"x\":90,\"y\":6335.169575012093},{\"x\":91,\"y\":6334.82704428686},{\"x\":92,\"y\":6334.491409806904},{\"x\":93,\"y\":6334.162477478907},{\"x\":94,\"y\":6333.840059842853},{\"x\":95,\"y\":6333.523975816073},{\"x\":96,\"y\":6333.214050447939},{\"x\":97,\"y\":6332.91011468477},{\"x\":98,\"y\":6332.612005144494},{\"x\":99,\"y\":6332.319563900635},{\"x\":100,\"y\":6332.032638275235}]},\"extension\":[]}\n",
       "        var id_name = '#vis-cdafbc03-1f8f-4335-9809-bff552bcb598';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x0000000003269a00 @properties={:diagrams=>[#<Nyaplot::Diagram:0x00000000032303e0 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"489ded59-5088-4d49-86d4-b78ae5399634\"}, @xrange=[1, 100], @yrange=[6332.032638275235, 6466.981467387201]>], :options=>{:x_label=>\"Iteration\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[1, 100], :yrange=>[6332.032638275235, 6466.981467387201]}}>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t44 = BetaBinomialModel.new\n",
    "t44_data = coin_dataset(10000)\n",
    "\n",
    "t44_w_init, t44_learning_rate = q44_weights_and_learning_rate()\n",
    "\n",
    "t44_total_loss = 0.0\n",
    "t44_iterations = []\n",
    "t44_losses = []\n",
    "t44_w = t34_w_init\n",
    "gradient_descent t44_data, t44_w_init, t44, t44_learning_rate, 0.001, 100 do |w, iter, loss|\n",
    "  puts [iter, w, t44_losses[-1]].join(\"\\t\") if iter % 10 == 1\n",
    "  t44_total_loss += loss\n",
    "  t44_iterations << iter\n",
    "  t44_losses << t44_total_loss / iter.to_f\n",
    "  t44_w = w\n",
    "end\n",
    "\n",
    "\n",
    "assert_true(t44_losses[-1] < 8000)\n",
    "assert_true(t44_iterations[-1] > 30)\n",
    "assert_in_delta 0.77, t44_w[\"bias\"], 5e-2\n",
    "\n",
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t44_iterations, y: t44_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
